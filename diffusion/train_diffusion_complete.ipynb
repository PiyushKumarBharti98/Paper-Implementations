{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Complete Standalone Diffusion Training Notebook\n",
                "\n",
                "This notebook contains all the necessary code to train a diffusion model on CIFAR-10 from scratch, including the model architecture and scheduler definitions. It is designed to be run directly on Kaggle."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "from torch.utils.data import DataLoader\n",
                "from torchvision import datasets, transforms\n",
                "from tqdm.auto import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Hyperparameters\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "BATCH_SIZE = 64\n",
                "LEARNING_RATE = 1e-4\n",
                "EPOCHS = 20\n",
                "IMAGE_SIZE = 32\n",
                "CHANNELS = 3\n",
                "N_TIMESTEPS = 1000\n",
                "N_HEADS = 4\n",
                "N_EMBED = 320\n",
                "SAVE_INTERVAL = 5 # Save checkpoint every 5 epochs\n",
                "\n",
                "print(f\"Using device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. DDPM Scheduler (Modified for Training)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DDPMSampler:\n",
                "    def __init__(\n",
                "        self,\n",
                "        generator: torch.Generator,\n",
                "        num_training_steps: int = 10000,\n",
                "        beta_start: float = 0.00085,\n",
                "        beta_end: float = 0.0120,\n",
                "    ) -> None:\n",
                "        self.beta = (\n",
                "            torch.linspace(\n",
                "                beta_start**0.5, beta_end**0.5, num_training_steps, dtype=torch.float32\n",
                "            )\n",
                "            ** 2\n",
                "        )\n",
                "        self.alpha = 1.0 - self.beta\n",
                "        self.generator = generator\n",
                "        self.one = torch.tensor(1.0)\n",
                "        self.alphacum = torch.cumprod(self.alpha, dim=0)\n",
                "        self.num_training_steps = num_training_steps\n",
                "        self.timestep = torch.from_numpy(np.arange(0, num_training_steps)[::-1].copy())\n",
                "\n",
                "    def set_inference_timesteps(self, num_inference_steps=50):\n",
                "        self.num_inference_steps = num_inference_steps\n",
                "        step_ratio = self.num_training_steps // self.num_inference_steps\n",
                "        timesteps = (\n",
                "            (np.arange(0, num_inference_steps) * step_ratio)\n",
                "            .round()[::-1]\n",
                "            .copy()\n",
                "            .astype(np.int64)\n",
                "        )\n",
                "        self.timesteps = torch.from_numpy(timesteps)\n",
                "\n",
                "    def _get_previous_timestep(self, timestep: int) -> int:\n",
                "        prev_timestep = timestep - (self.num_training_steps // self.num_inference_steps)\n",
                "        return prev_timestep\n",
                "\n",
                "    def _get_variance(self, timestep: int):\n",
                "        previous_t = self._get_previous_timestep(timestep)\n",
                "        alpha_t = self.alphacum[timestep]\n",
                "        alpha_prev_t = self.alphacum[previous_t] if previous_t >= 0 else self.one\n",
                "        current_beta_t = 1 - alpha_t / alpha_prev_t\n",
                "\n",
                "        variance = ((1 - alpha_prev_t) / (1 - alpha_t)) * current_beta_t\n",
                "        variance = torch.clamp(variance, min=1e-10)\n",
                "\n",
                "        return variance\n",
                "\n",
                "    def set_strength(self, strength: float = 1.0):\n",
                "        start_step = self.num_training_steps - (self.num_inference_steps * strength)\n",
                "        self.timesteps = self.timesteps[start_step:]\n",
                "        self.start_step = start_step\n",
                "\n",
                "    def step(self, timestep: int, latents: torch.Tensor, model_outputs: torch.Tensor):\n",
                "        t = timestep\n",
                "        previous_t = self._get_previous_timestep(t)\n",
                "\n",
                "        alpha_t = self.alphacum[timestep]\n",
                "        alpha_prev_t = self.alphacum[previous_t] if previous_t >= 0 else self.one\n",
                "        beta_t = 1 - alpha_t\n",
                "        beta_prev_t = 1 - alpha_prev_t\n",
                "        curr_alpha_t = alpha_t / alpha_prev_t\n",
                "        curr_beta_t = 1 - curr_alpha_t\n",
                "\n",
                "        # x0 calculation\n",
                "        prediction_original_sample = (\n",
                "            latents - (beta_t ** (0.5)) * model_outputs\n",
                "        ) / alpha_t**0.5\n",
                "\n",
                "        predicted_sample_coeff = ((beta_prev_t ** (0.5)) * curr_beta_t) / beta_t\n",
                "        current_sample_coeff = (curr_alpha_t) ** 0.5 * (beta_prev_t) / curr_beta_t\n",
                "\n",
                "        predicted_prev_samples = (\n",
                "            predicted_sample_coeff * prediction_original_sample\n",
                "            + current_sample_coeff * latents\n",
                "        )\n",
                "\n",
                "        variance = 0\n",
                "        if t > 0:\n",
                "            device = model_outputs.device\n",
                "            noise = torch.randn(\n",
                "                model_outputs.shape,\n",
                "                generator=self.generator,\n",
                "                device=device,\n",
                "                dtype=model_outputs.dtype,\n",
                "            )\n",
                "\n",
                "            variance = (self._get_variance(t) ** 0.5) * noise\n",
                "            predicted_prev_samples += variance\n",
                "\n",
                "        return predicted_prev_samples\n",
                "\n",
                "    def add_noise(\n",
                "        self, original_samples: torch.FloatTensor, timesteps: torch.IntTensor\n",
                "    ):\n",
                "        cumprod_add = self.alphacum.to(\n",
                "            device=original_samples.device, dtype=original_samples.dtype\n",
                "        )\n",
                "        timesteps = timesteps.to(original_samples.device)\n",
                "\n",
                "        sqrt_alpha = self.alphacum[timesteps] ** 0.5\n",
                "        sqrt_alpha = sqrt_alpha.flatten()\n",
                "        if len(sqrt_alpha.shape) < len(original_samples.shape):\n",
                "            sqrt_alpha = sqrt_alpha.unsqueeze(-1)\n",
                "\n",
                "        sqrt_alpha_minus = (1 - self.alphacum[timesteps]) ** 0.5\n",
                "        sqrt_alpha_minus = sqrt_alpha_minus.flatten()\n",
                "        if len(sqrt_alpha_minus.shape) < len(original_samples.shape):\n",
                "            sqrt_alpha_minus = sqrt_alpha_minus.unsqueeze(-1)\n",
                "\n",
                "        noise = torch.randn(\n",
                "            original_samples.shape,\n",
                "            generator=self.generator,\n",
                "            device=original_samples.device,\n",
                "            dtype=original_samples.dtype,\n",
                "        )\n",
                "\n",
                "        final_noise = sqrt_alpha * original_samples + sqrt_alpha_minus * noise\n",
                "        return final_noise, noise  # Modified to return noise for training loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Architecture (Modified for RGB Input)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SelfAttention(nn.Module):\n",
                "    def __init__(\n",
                "        self,\n",
                "        n_heads: int,\n",
                "        d_embed: int,\n",
                "        input_projection_bias: bool = True,\n",
                "        output_projection_bias: bool = True,\n",
                "    ) -> None:\n",
                "        super().__init__()\n",
                "        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=input_projection_bias)\n",
                "        self.out_proj = nn.Linear(d_embed, d_embed, bias=output_projection_bias)\n",
                "\n",
                "        self.n_heads = n_heads\n",
                "        self.d_head = d_embed // n_heads\n",
                "\n",
                "    def forward(self, x, masked=False):\n",
                "\n",
                "        batch_size, seq_len, d_embed = x.shape\n",
                "\n",
                "        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
                "\n",
                "        # manual forward pass\n",
                "        q = q.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
                "        k = k.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
                "        v = v.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
                "\n",
                "        scale = math.sqrt(self.d_head)\n",
                "\n",
                "        qk = q @ k.transpose(-1, -2)\n",
                "\n",
                "        if masked == True:\n",
                "            mask = torch.ones(seq_len, seq_len, dtype=bool, device=qk.device).triu(\n",
                "                diagonal=1\n",
                "            )\n",
                "            qk = qk.masked_fill(mask)\n",
                "\n",
                "        mul = qk / scale\n",
                "\n",
                "        final = F.softmax(qk, dim=-1)\n",
                "\n",
                "        output = final @ v\n",
                "\n",
                "        output = output.transpose(1, 2).reshape(batch_size, seq_len, d_embed)\n",
                "\n",
                "        output = self.out_proj(output)\n",
                "\n",
                "        return output\n",
                "\n",
                "\n",
                "class CrossAttention(nn.Module):\n",
                "    def __init__(\n",
                "        self,\n",
                "        n_heads: int,\n",
                "        d_embed: int,\n",
                "        d_cross: int,\n",
                "        input_projection_bias: bool = True,\n",
                "        output_projection_bias: bool = True,\n",
                "    ) -> None:\n",
                "        super().__init__()\n",
                "        self.q_proj = nn.Linear(d_embed, d_embed, bias=input_projection_bias)\n",
                "        self.k_proj = nn.Linear(d_cross, d_embed, bias=input_projection_bias)\n",
                "        self.v_proj = nn.Linear(d_cross, d_embed, bias=input_projection_bias)\n",
                "        self.out_proj = nn.Linear(d_embed, d_embed, bias=output_projection_bias)\n",
                "\n",
                "        self.n_heads = n_heads\n",
                "        self.d_head = d_embed // n_heads\n",
                "\n",
                "    def forward(self, x, y, masked=False):\n",
                "\n",
                "        batch_size, seq_len, d_embed = x.shape\n",
                "\n",
                "        q = self.q_proj(x)\n",
                "        k = self.k_proj(y)\n",
                "        v = self.v_proj(y)\n",
                "\n",
                "        # manual forward pass\n",
                "        q = q.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
                "        k = k.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
                "        v = v.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
                "\n",
                "        scale = math.sqrt(self.d_head)\n",
                "\n",
                "        qk = q @ k.transpose(-1, -2)\n",
                "\n",
                "        if masked == True:\n",
                "            mask = torch.ones(seq_len, seq_len, dtype=bool, device=qk.device).triu(\n",
                "                diagonal=1\n",
                "            )\n",
                "            qk = qk.masked_fill(mask)\n",
                "\n",
                "        mul = qk / scale\n",
                "\n",
                "        final = F.softmax(qk, dim=-1)\n",
                "\n",
                "        output = final @ v\n",
                "\n",
                "        output = output.transpose(1, 2).reshape(batch_size, seq_len, d_embed)\n",
                "\n",
                "        output = self.out_proj(output)\n",
                "\n",
                "        return output\n",
                "\n",
                "\n",
                "class TimeEmbedding(nn.Module):\n",
                "    def __init__(self, n_embed: int) -> None:\n",
                "        super().__init__()\n",
                "        self.linear1 = nn.Linear(n_embed, 4 * n_embed)\n",
                "        self.linear2 = nn.Linear(4 * n_embed, 4 * n_embed)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.linear1(x)\n",
                "        x = F.silu(x)\n",
                "        x = self.linear2(x)\n",
                "        return x\n",
                "\n",
                "\n",
                "class UNET_AttentionBlock(nn.Module):\n",
                "    def __init__(self, n_heads: int, n_embed: int, d_cross: int = 768) -> None:\n",
                "        super().__init__()\n",
                "        channels = n_heads * n_embed\n",
                "\n",
                "        self.groupnorm = nn.GroupNorm(32, channels)\n",
                "        self.convlayer = nn.Conv2d(channels, channels, kernel_size=1, padding=0)\n",
                "\n",
                "        self.layernorm1 = nn.LayerNorm(channels)\n",
                "        self.selfattention = SelfAttention(n_heads, n_embed)\n",
                "\n",
                "        self.layernorm2 = nn.LayerNorm(channels)\n",
                "        self.crosssattention = CrossAttention(n_heads, n_embed, d_cross=d_cross) # Added d_cross to init call if not hardcoded\n",
                "\n",
                "        self.layernorm3 = nn.LayerNorm(channels)\n",
                "        self.linear1 = nn.Linear(channels, 4 * channels)\n",
                "        self.linear2 = nn.Linear(4 * channels, channels)\n",
                "\n",
                "        self.convout = nn.Conv2d(channels, channels, kernel_size=1, padding=0)\n",
                "\n",
                "    def forward(self, x, context):\n",
                "\n",
                "        long_residue = x\n",
                "\n",
                "        x = self.groupnorm(x)\n",
                "        x = self.convlayer(x)\n",
                "\n",
                "        n, c, h, w = x.shape\n",
                "\n",
                "        x = x.view(n, c, h * w).transpose(-1, -2)\n",
                "\n",
                "        short_residue = x\n",
                "\n",
                "        x = self.layernorm1(x)\n",
                "        x = self.selfattention(x)\n",
                "\n",
                "        x += short_residue\n",
                "\n",
                "        short_residue = x\n",
                "\n",
                "        x = self.layernorm2(x)\n",
                "        x = self.crosssattention(x, context)\n",
                "\n",
                "        x += short_residue\n",
                "\n",
                "        short_residue = x\n",
                "\n",
                "        x = self.layernorm3(x)\n",
                "        x, gate = self.linear1(x).chunk(2, dim=-1)\n",
                "        x = x * F.gelu(gate)\n",
                "        x = self.linear2(x)\n",
                "\n",
                "        x += short_residue\n",
                "\n",
                "        x = x.transpose(-1, -2).view((n, c, h, w))\n",
                "\n",
                "        x = self.convout(x)\n",
                "\n",
                "        return x + long_residue\n",
                "\n",
                "\n",
                "class Upsample(nn.Module):\n",
                "    def __init__(\n",
                "        self,\n",
                "        channels: int,\n",
                "    ) -> None:\n",
                "        super().__init__()\n",
                "        self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
                "\n",
                "    def forward(self, x):\n",
                "\n",
                "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
                "        x = self.conv(x)\n",
                "        return x\n",
                "\n",
                "\n",
                "class UNET_ResidualBlock(nn.Module):\n",
                "    def __init__(self, in_channels: int, out_channels: int, n_time: int = 1280) -> None:\n",
                "        super().__init__()\n",
                "        self.groupnorm = nn.GroupNorm(32, in_channels)\n",
                "        self.convlayer = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
                "\n",
                "        self.timelayer = nn.Linear(n_time, out_channels)\n",
                "        \n",
                "        # Fix: Ensure GroupNorm num_groups matches channels\n",
                "        # The original code uses 32 groups. If out_channels < 32, this will crash.\n",
                "        # For standard UNET sizes (320, 640, 1280), it's fine.\n",
                "        self.groupnorm_time = nn.GroupNorm(32, out_channels)\n",
                "        self.convlayer_time = nn.Conv2d(\n",
                "            in_channels, out_channels, kernel_size=3, padding=1\n",
                "        )\n",
                "\n",
                "        if in_channels == out_channels:\n",
                "            self.residual_layer = nn.Identity()\n",
                "        else:\n",
                "            self.residual_layer = nn.Conv2d(\n",
                "                in_channels, out_channels, kernel_size=1, padding=0\n",
                "            )\n",
                "\n",
                "    def forward(self, image: torch.Tensor, time):\n",
                "\n",
                "        residue = image\n",
                "\n",
                "        image = self.groupnorm(image)\n",
                "        image = F.silu(image)\n",
                "        image = self.convlayer(image)\n",
                "\n",
                "        time = self.timelayer(time)\n",
                "\n",
                "        # Using unsqueeze to match dimensions\n",
                "        y = image + time.unsqueeze(-1).unsqueeze(-1)\n",
                "        y = self.groupnorm_time(y)\n",
                "        y = F.silu(y)\n",
                "        y = self.convlayer_time(y)\n",
                "\n",
                "        return y + self.residual_layer(residue)\n",
                "\n",
                "\n",
                "class SwitchSequential(nn.Module):\n",
                "    def __init__(self, *layers):\n",
                "        super().__init__()\n",
                "        self.layers = nn.ModuleList(layers)\n",
                "\n",
                "    def forward(self, x, context, time):\n",
                "        for layer in self.layers:\n",
                "            if isinstance(layer, UNET_ResidualBlock):\n",
                "                x = layer(x, time)\n",
                "            elif isinstance(layer, UNET_AttentionBlock):\n",
                "                x = layer(x, context)\n",
                "            else:\n",
                "                x = layer(x)\n",
                "        return x\n",
                "\n",
                "\n",
                "class UNET(nn.Module):\n",
                "    def __init__(self, in_channels: int = 4) -> None:\n",
                "        super().__init__()\n",
                "\n",
                "        self.encoder = nn.ModuleList(\n",
                "            [\n",
                "                SwitchSequential(nn.Conv2d(in_channels, 320, kernel_size=3, padding=1)), # Modified for in_channels\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(320, 320), UNET_AttentionBlock(8, 40)\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(320, 320), UNET_AttentionBlock(8, 40)\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    nn.Conv2d(320, 320, kernel_size=3, stride=2, padding=1)\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(320, 640), UNET_AttentionBlock(8, 80)\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(640, 640), UNET_AttentionBlock(8, 80)\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    nn.Conv2d(640, 640, kernel_size=3, stride=2, padding=1)\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(640, 1280), UNET_AttentionBlock(8, 160)\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(1280, 1280), UNET_AttentionBlock(8, 160)\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    nn.Conv2d(1280, 1280, kernel_size=3, stride=2, padding=1)\n",
                "                ),\n",
                "                SwitchSequential(UNET_ResidualBlock(1280, 1280)),\n",
                "                SwitchSequential(UNET_ResidualBlock(1280, 1280)),\n",
                "            ]\n",
                "        )\n",
                "\n",
                "        self.bottleneck = nn.ModuleList(\n",
                "            [\n",
                "                UNET_ResidualBlock(1280, 1280),\n",
                "                UNET_AttentionBlock(8, 160),\n",
                "                UNET_ResidualBlock(1280, 1280),\n",
                "            ]\n",
                "        )\n",
                "\n",
                "        self.decoder = nn.ModuleList(\n",
                "            [\n",
                "                SwitchSequential(UNET_ResidualBlock(2560, 1280)),\n",
                "                SwitchSequential(UNET_ResidualBlock(2560, 1280)),\n",
                "                SwitchSequential(UNET_ResidualBlock(2560, 1280), Upsample(1280)),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(2560, 1280), UNET_AttentionBlock(8, 160)\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(2560, 1280), UNET_AttentionBlock(8, 160)\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(2560, 1280), UNET_AttentionBlock(8, 160)\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(1920, 1280),\n",
                "                    UNET_AttentionBlock(8, 160),\n",
                "                    Upsample(1280),\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(1920, 640),\n",
                "                    UNET_AttentionBlock(8, 80),\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(1920, 640),\n",
                "                    UNET_AttentionBlock(8, 80),\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(1280, 640),\n",
                "                    UNET_AttentionBlock(8, 80),\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(1280, 640),\n",
                "                    UNET_AttentionBlock(8, 80),\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(960, 640),\n",
                "                    UNET_AttentionBlock(8, 80),\n",
                "                    Upsample(640),\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(960, 320),\n",
                "                    UNET_AttentionBlock(8, 80),\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(640, 320),\n",
                "                    UNET_AttentionBlock(8, 80),\n",
                "                ),\n",
                "                SwitchSequential(\n",
                "                    UNET_ResidualBlock(640, 320),\n",
                "                    UNET_AttentionBlock(8, 80),\n",
                "                ),\n",
                "            ]\n",
                "        )\n",
                "\n",
                "    def forward(self, x, context, time):\n",
                "\n",
                "        skip_connections = []\n",
                "        for layer in self.encoder:\n",
                "            x = layer(x, context, time)\n",
                "            skip_connections.append(x)\n",
                "\n",
                "        x = self.bottleneck(x, context, time)\n",
                "\n",
                "        for layer in self.decoder:\n",
                "            x = torch.cat((x, skip_connections.pop()), dim=1)\n",
                "            x = layer(x, context, time)\n",
                "\n",
                "        return x\n",
                "\n",
                "\n",
                "class UNET_Out(nn.Module):\n",
                "    def __init__(self, in_channels, out_channels) -> None:\n",
                "        super().__init__()\n",
                "        self.groupnorm = nn.GroupNorm(32, in_channels)\n",
                "        self.convlayer = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1) # Standard 3x3 out\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.groupnorm(x)\n",
                "        x = F.silu(x)\n",
                "        x = self.convlayer(x)\n",
                "\n",
                "        return x\n",
                "\n",
                "\n",
                "class Diffusion(nn.Module):\n",
                "    def __init__(\n",
                "        self,\n",
                "        n_channels: int = 4\n",
                "    ) -> None:\n",
                "        super().__init__()\n",
                "        self.time = TimeEmbedding(320)\n",
                "        self.unet = UNET(in_channels=n_channels)\n",
                "        self.output = UNET_Out(320, n_channels)\n",
                "\n",
                "    def forward(self, x, context, time):\n",
                "\n",
                "        time = self.time(time)\n",
                "\n",
                "        out = self.unet(x, context, time)\n",
                "\n",
                "        out = self.output(out)\n",
                "\n",
                "        return out"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Dataset Loading (CIFAR-10) with Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "transform = transforms.Compose([\n",
                "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
                "])\n",
                "\n",
                "print(\"Loading Dataset...\")\n",
                "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
                "\n",
                "# --- Verification Logic ---\n",
                "print(\"\\n--- Verifying Dataset ---\")\n",
                "print(f\"Total samples: {len(train_dataset)}\")\n",
                "\n",
                "# Check one batch\n",
                "sample_batch, sample_labels = next(iter(train_loader))\n",
                "print(f\"Batch Shape: {sample_batch.shape}\")\n",
                "print(f\"Expected Shape: ({BATCH_SIZE}, {CHANNELS}, {IMAGE_SIZE}, {IMAGE_SIZE})\")\n",
                "\n",
                "assert sample_batch.shape[1] == CHANNELS, f\"Channel mismatch! Expected {CHANNELS}, got {sample_batch.shape[1]}\"\n",
                "assert sample_batch.shape[2] == IMAGE_SIZE, f\"Height mismatch! Expected {IMAGE_SIZE}, got {sample_batch.shape[2]}\"\n",
                "assert sample_batch.shape[3] == IMAGE_SIZE, f\"Width mismatch! Expected {IMAGE_SIZE}, got {sample_batch.shape[3]}\"\n",
                "\n",
                "print(f\"Value Range: Min {sample_batch.min():.4f}, Max {sample_batch.max():.4f}\")\n",
                "\n",
                "if sample_batch.shape[1] == 3:\n",
                "    # Visualize a grid\n",
                "    print(\"Visualizing Sample Images...\")\n",
                "    grid_img = sample_batch[:8] # Take first 8\n",
                "    grid_img = (grid_img * 0.5 + 0.5).permute(0, 2, 3, 1).numpy() # Denormalize\n",
                "    \n",
                "    fig, axes = plt.subplots(1, 8, figsize=(12, 2))\n",
                "    for i, ax in enumerate(axes):\n",
                "        ax.imshow(grid_img[i])\n",
                "        ax.axis('off')\n",
                "    plt.show()\n",
                "\n",
                "print(\"--- Dataset Verification Passed ---\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training Loop with Logging and Checkpointing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Diffusion Model\n",
                "print(\"Initializing Model...\")\n",
                "model = Diffusion(n_channels=CHANNELS).to(DEVICE)\n",
                "\n",
                "# Verify Model Compilation\n",
                "dummy_input = torch.randn(1, CHANNELS, IMAGE_SIZE, IMAGE_SIZE).to(DEVICE)\n",
                "dummy_context = torch.zeros(1, 77, 768).to(DEVICE)\n",
                "dummy_time = torch.tensor([1]).to(DEVICE)\n",
                "try:\n",
                "    _ = model(dummy_input, dummy_context, dummy_time)\n",
                "    print(\"Model forward pass check: SUCCESS\")\n",
                "except Exception as e:\n",
                "    print(f\"Model forward pass check: FAILED with error: {e}\")\n",
                "    raise e\n",
                "\n",
                "# Optimizer and Loss\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
                "criterion = nn.MSELoss()\n",
                "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
                "\n",
                "# DDPM Sampler\n",
                "generator = torch.Generator(device=DEVICE)\n",
                "sampler = DDPMSampler(generator, num_training_steps=N_TIMESTEPS)\n",
                "\n",
                "# Training\n",
                "print(\"Starting Training...\")\n",
                "output_history = {\"loss\": []}\n",
                "\n",
                "try:\n",
                "    for epoch in range(EPOCHS):\n",
                "        model.train()\n",
                "        epoch_loss = 0\n",
                "        \n",
                "        # Progress bar for the epoch\n",
                "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
                "        \n",
                "        for step, (images, _) in enumerate(pbar):\n",
                "            # Index verification log (every 100 steps)\n",
                "            if step % 200 == 0:\n",
                "                # Just a sanity print to ensure loop is progressing correctly\n",
                "                pass # tqdm handles display, but we can verify data integrity if needed\n",
                "                \n",
                "            images = images.to(DEVICE)\n",
                "            batch_size = images.shape[0]\n",
                "            \n",
                "            # Sample random timesteps\n",
                "            t = torch.randint(0, N_TIMESTEPS, (batch_size,), device=DEVICE).long()\n",
                "            \n",
                "            # Add noise to images\n",
                "            noisy_images, noise = sampler.add_noise(images, t)\n",
                "            \n",
                "            # Create unconditional context (null tokens)\n",
                "            context = torch.zeros(batch_size, 77, 768).to(DEVICE)\n",
                "            \n",
                "            # Forward pass\n",
                "            noise_pred = model(noisy_images, context, t)\n",
                "            \n",
                "            # Calculate loss\n",
                "            loss = criterion(noise_pred, noise)\n",
                "            \n",
                "            # Backward pass\n",
                "            optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            epoch_loss += loss.item()\n",
                "            pbar.set_postfix({\"Loss\": loss.item()})\n",
                "        \n",
                "        scheduler.step()\n",
                "        avg_loss = epoch_loss / len(train_loader)\n",
                "        output_history[\"loss\"].append(avg_loss)\n",
                "        print(f\"Epoch {epoch+1} Completed. Average Loss: {avg_loss:.4f}\")\n",
                "        \n",
                "        # Save Checkpoint Logic\n",
                "        if (epoch + 1) % SAVE_INTERVAL == 0:\n",
                "            print(f\"Saving checkpoint at epoch {epoch+1}...\")\n",
                "            checkpoint = {\n",
                "                'epoch': epoch + 1,\n",
                "                'model_state_dict': model.state_dict(),\n",
                "                'optimizer_state_dict': optimizer.state_dict(),\n",
                "                'loss': avg_loss,\n",
                "            }\n",
                "            torch.save(checkpoint, f\"diffusion_checkpoint_epoch_{epoch+1}.pt\")\n",
                "            print(f\"Checkpoint saved: diffusion_checkpoint_epoch_{epoch+1}.pt\")\n",
                "\n",
                "except KeyboardInterrupt:\n",
                "    print(\"Training interrupted by user. Saving emergency checkpoint...\")\n",
                "    torch.save(model.state_dict(), \"diffusion_emergency_checkpoint.pt\")\n",
                "    print(\"Emergency checkpoint saved.\")\n",
                "except Exception as e:\n",
                "    print(f\"Error occurred: {e}. Saving emergency checkpoint...\")\n",
                "    torch.save(model.state_dict(), \"diffusion_error_checkpoint.pt\")\n",
                "    raise e\n",
                "\n",
                "print(\"Training process finished.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}